<!DOCTYPE html><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"><script type="module">const e=document.getElementById("link-menu-button"),n=document.getElementById("link-menu"),i=document.getElementById("open-indicator");e.addEventListener("click",()=>{n.classList.contains("opacity-0")?(n.classList.remove("pointer-events-none"),n.classList.remove("opacity-0"),i.innerText="-"):(n.classList.add("pointer-events-none"),n.classList.add("opacity-0"),i.innerText="+")});document.addEventListener("click",t=>{const s=t.target===e||e.contains(t.target),c=t.target===n||n.contains(t.target);!s&&!c&&(n.classList.add("pointer-events-none"),n.classList.add("opacity-0"),i.innerText="+")});</script> <html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.0.0"><title>Machine Learning</title><meta name="description" content="This Machine Learning module provides a practical, hands-on introduction to machine learning concepts, algorithms, and frameworks, emphasizing real-world data analytics, systematic implementation, and the ethical, professional, and collaborative aspects of deploying machine learning solutions."><meta property="og:title" content="Machine Learning"><meta property="og:description" content="This Machine Learning module provides a practical, hands-on introduction to machine learning concepts, algorithms, and frameworks, emphasizing real-world data analytics, systematic implementation, and the ethical, professional, and collaborative aspects of deploying machine learning solutions."><meta property="og:image" content><link rel="stylesheet" href="/MSc-Artificial-Intelligence-Portfolio/_astro/about.msOe3G2r.css">
<style>@import"https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap";html{font-family:Inter,sans-serif;background:#000}*{margin:0;padding:0;box-sizing:border-box;-webkit-font-smoothing:antialiased;color:#fff}code{font-family:Menlo,Monaco,Lucida Console,Liberation Mono,DejaVu Sans Mono,Bitstream Vera Sans Mono,Courier New,monospace}.hover-underline{-webkit-text-decoration:underline 1px rgba(0,0,0,0);text-decoration:underline 1px rgba(0,0,0,0);text-underline-offset:5px;transition:text-decoration-color .3s}.post:hover .hover-underline{text-decoration-color:#fff}
[data-astro-image]{width:100%;height:auto;-o-object-fit:var(--fit);object-fit:var(--fit);-o-object-position:var(--pos);object-position:var(--pos);aspect-ratio:var(--w) / var(--h)}[data-astro-image=responsive]{max-width:calc(var(--w) * 1px);max-height:calc(var(--h) * 1px)}[data-astro-image=fixed]{width:calc(var(--w) * 1px);height:calc(var(--h) * 1px)}
</style></head> <body> <header> <div class="px-6 py-6 max-w-2xl mx-auto flex items-center justify-between"> <a href="/MSc-Artificial-Intelligence-Portfolio/" class="justify-content: center"> <img src="/MSc-Artificial-Intelligence-Portfolio/logo.jpeg" alt="logo" class="rounded-full" width="50" height="50" loading="lazy" decoding="async" class="rounded-full"> </a> <nav class="ml-auto mr-6 sm:m-0 flex justify-content: center"> <ul class="hidden sm:flex sm:items-center sm:gap-2"> <li> <a class="group px-4 py-2 rounded-lg hover:bg-gray-bg opacity-60 hover:opacity-100 transition-all" href="/MSc-Artificial-Intelligence-Portfolio/" target="_self"> Home </a> </li><li> <a class="group px-4 py-2 rounded-lg hover:bg-gray-bg opacity-60 hover:opacity-100 transition-all" href="/MSc-Artificial-Intelligence-Portfolio/about" target="_self"> About </a> </li><li> <a class="group px-4 py-2 rounded-lg hover:bg-gray-bg opacity-60 hover:opacity-100 transition-all" href="/MSc-Artificial-Intelligence-Portfolio/modules" target="_self"> Master&#39;s Modules </a> </li> </ul> <div class="relative sm:hidden"> <button id="link-menu-button" class="relative font-medium opacity-60 p-4">Menu <span id="open-indicator">+</span></button> <div id="link-menu" class="absolute p-2 right-0 bg-gray-bg rounded-lg w-44 transition-opacity ease-in-out duration-300 opacity-0 z-40"> <ul> <li> <a class="block pl-4 py-3" href="/MSc-Artificial-Intelligence-Portfolio/" target="_self"> Home </a> </li><li> <a class="block pl-4 py-3" href="/MSc-Artificial-Intelligence-Portfolio/about" target="_self"> About </a> </li><li> <a class="block pl-4 py-3" href="/MSc-Artificial-Intelligence-Portfolio/modules" target="_self"> Master&#39;s Modules </a> </li> </ul> </div> </div> </nav> <div> <a href="mailto:disnil7@gmail.com" class="block p-2 opacity-60 hover:opacity-100 transition-opacity" aria-label="Email"> <svg width="20" height="20" data-icon="mdi:email">   <symbol id="ai:mdi:email" viewBox="0 0 24 24"><path fill="currentColor" d="m20 8l-8 5l-8-5V6l8 5l8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"/></symbol><use href="#ai:mdi:email"></use>  </svg> </a> </div> </div> </header> <main class="mb-32 max-w-4xl mx-auto px-8"> <section class="mt-10 mx-auto px-6"><h1 class="font-bold text-3xl mb-1">Perceptron Activities</h1><p class="text-sm font-semibold text-xl mt-4 text-gray-400">Unit 7</p><div class="mt-6 prose prose-invert"><h2 id="single-perceptron">Single Perceptron</h2>
<p>This exercise demonstrates the basic operations of a single perceptron: calculating the dot product as a weighted sum of inputs and passing it through a step function as the activation mechanism.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="py"><code><span class="line"><span style="color:#6A737D"># Importing NumPy</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> numpy </span><span style="color:#F97583">as</span><span style="color:#E1E4E8"> np</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Exercise 1: Dot Product and Step Function</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;---- Exercise 1 ----&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Inputs and weights for the perceptron</span></span>
<span class="line"><span style="color:#6A737D"># Inputs represent features, and weights represent their importance.</span></span>
<span class="line"><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#79B8FF">45</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">25</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">weights </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#79B8FF">0.7</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Dot product simulates the sum function in a perceptron</span></span>
<span class="line"><span style="color:#E1E4E8">sum_func </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.dot(inputs, weights)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Step function simulates the perceptron&#39;s activation function</span></span>
<span class="line"><span style="color:#6A737D"># If the weighted sum is greater than or equal to a threshold, it activates (returns 1).</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> step_function</span><span style="color:#E1E4E8">(value):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> if</span><span style="color:#E1E4E8"> value </span><span style="color:#F97583">&gt;=</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> else</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">result_step </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> step_function(sum_func)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">&quot;Sum: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">sum_func</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">, Step Function Result: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">result_step</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">&quot;</span><span style="color:#E1E4E8">)</span></span></code></pre>
<h2 id="training-a-perceptron-and-operator">Training a Perceptron (AND Operator)</h2>
<p>This follow up exercise introduces training a perceptron to learn the logic of an AND gate. The weights are adjusted iteratively based on errors until the perceptron accurately models the desired behavior.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="py"><code><span class="line"><span style="color:#6A737D"># Exercise 2: Training a Perceptron</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">---- Exercise 2 ----&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Inputs and expected outputs for an AND gate</span></span>
<span class="line"><span style="color:#6A737D"># Each row in inputs represents a data point, and outputs is the target result.</span></span>
<span class="line"><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]])</span></span>
<span class="line"><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># AND gate logic</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Initialize weights to zero and set a learning rate</span></span>
<span class="line"><span style="color:#E1E4E8">weights </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#E1E4E8">learning_rate </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.1</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Perceptron training function</span></span>
<span class="line"><span style="color:#6A737D"># Adjusts weights based on errors to learn the AND gate logic.</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> train_perceptron</span><span style="color:#E1E4E8">():</span></span>
<span class="line"><span style="color:#F97583">    global</span><span style="color:#E1E4E8"> weights</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># Fixed number of epochs (iterations over the data)</span></span>
<span class="line"><span style="color:#E1E4E8">        total_error </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">len</span><span style="color:#E1E4E8">(outputs)):</span></span>
<span class="line"><span style="color:#6A737D">            # Calculate the perceptron&#39;s prediction</span></span>
<span class="line"><span style="color:#E1E4E8">            prediction </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> step_function(np.dot(inputs[i], weights))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">            # Error is the difference between the target and the prediction</span></span>
<span class="line"><span style="color:#E1E4E8">            error </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> outputs[i] </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> prediction</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">            # Update weights based on the error and learning rate</span></span>
<span class="line"><span style="color:#E1E4E8">            weights </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> learning_rate </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> inputs[i] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> error</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">            # Track total error to decide when to stop</span></span>
<span class="line"><span style="color:#E1E4E8">            total_error </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> abs</span><span style="color:#E1E4E8">(error)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">        # Stop early if the perceptron has no errors</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> total_error </span><span style="color:#F97583">==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">            break</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">&quot;Trained Weights: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">weights</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">train_perceptron()</span></span></code></pre>
<h2 id="multilayer-perceptron">Multilayer Perceptron</h2>
<p>Finally, this experiment implements a simple artificial neural network to solve the XOR problem, which cannot be addressed by a single perceptron. The network uses a sigmoid activation function, forward propagation, and backpropagation to adjust weights over multiple epochs.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8;overflow-x:auto" tabindex="0" data-language="py"><code><span class="line"><span style="color:#6A737D"># Exercise 3: Simple Artificial Neural Network</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">---- Exercise 3 ----&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Define sigmoid activation function</span></span>
<span class="line"><span style="color:#6A737D"># Maps any real value into the range (0, 1), enabling the ANN to handle non-linear problems.</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> sigmoid</span><span style="color:#E1E4E8">(x):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> 1</span><span style="color:#F97583"> /</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">1</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> np.exp(</span><span style="color:#F97583">-</span><span style="color:#E1E4E8">x))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Derivative of sigmoid (used during backpropagation)</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> sigmoid_derivative</span><span style="color:#E1E4E8">(x):</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> x </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> (</span><span style="color:#79B8FF">1</span><span style="color:#F97583"> -</span><span style="color:#E1E4E8"> x)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Inputs and outputs for XOR gate</span></span>
<span class="line"><span style="color:#6A737D"># XOR cannot be solved by a single perceptron, hence a neural network is needed.</span></span>
<span class="line"><span style="color:#E1E4E8">inputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">]])</span></span>
<span class="line"><span style="color:#E1E4E8">outputs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.array([[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">], [</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">]])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Initialize weights randomly for input-to-hidden and hidden-to-output layers</span></span>
<span class="line"><span style="color:#E1E4E8">weights_0 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.random.uniform(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, (</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">))  </span><span style="color:#6A737D"># 2 inputs, 3 hidden neurons</span></span>
<span class="line"><span style="color:#E1E4E8">weights_1 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.random.uniform(</span><span style="color:#F97583">-</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">, (</span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">))  </span><span style="color:#6A737D"># 3 hidden neurons, 1 output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Learning rate controls the size of weight updates</span></span>
<span class="line"><span style="color:#E1E4E8">learning_rate </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 0.1</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Train the neural network</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> epoch </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10000</span><span style="color:#E1E4E8">):  </span><span style="color:#6A737D"># Number of training iterations</span></span>
<span class="line"><span style="color:#6A737D">    # Forward pass: compute the outputs of each layer</span></span>
<span class="line"><span style="color:#E1E4E8">    hidden_layer_input </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.dot(inputs, weights_0)</span></span>
<span class="line"><span style="color:#E1E4E8">    hidden_layer_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sigmoid(hidden_layer_input)</span></span>
<span class="line"><span style="color:#E1E4E8">    final_layer_input </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> np.dot(hidden_layer_output, weights_1)</span></span>
<span class="line"><span style="color:#E1E4E8">    final_output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sigmoid(final_layer_input)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Calculate error at the output layer</span></span>
<span class="line"><span style="color:#E1E4E8">    error </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> outputs </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> final_output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Backpropagation: compute adjustments for weights</span></span>
<span class="line"><span style="color:#E1E4E8">    final_output_delta </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> error </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> sigmoid_derivative(final_output)</span></span>
<span class="line"><span style="color:#E1E4E8">    hidden_layer_error </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> final_output_delta.dot(weights_1.T)</span></span>
<span class="line"><span style="color:#E1E4E8">    hidden_layer_delta </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> hidden_layer_error </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> sigmoid_derivative(hidden_layer_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Update weights based on deltas</span></span>
<span class="line"><span style="color:#E1E4E8">    weights_1 </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> hidden_layer_output.T.dot(final_output_delta) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> learning_rate</span></span>
<span class="line"><span style="color:#E1E4E8">    weights_0 </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> inputs.T.dot(hidden_layer_delta) </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> learning_rate</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Optional: Print the error every 1000 epochs for monitoring</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> epoch </span><span style="color:#F97583">%</span><span style="color:#79B8FF"> 1000</span><span style="color:#F97583"> ==</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">        print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">&quot;Epoch </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">epoch</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">, Error: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">np.mean(np.abs(error))</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Final predictions after training</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">&quot;</span><span style="color:#79B8FF">\n</span><span style="color:#9ECBFF">Final Predictions:&quot;</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> i, input_val </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> enumerate</span><span style="color:#E1E4E8">(inputs):</span></span>
<span class="line"><span style="color:#E1E4E8">    hidden </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sigmoid(np.dot(input_val, weights_0))</span></span>
<span class="line"><span style="color:#E1E4E8">    output </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sigmoid(np.dot(hidden, weights_1))</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">&quot;Input: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">input_val</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">, Predicted: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">output.round()</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">&quot;</span><span style="color:#E1E4E8">)</span></span></code></pre></div></section> </main> <footer></footer> </body></html>